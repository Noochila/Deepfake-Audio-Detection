{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40d35642-b71e-403e-827e-bdc8ac428f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (0.10.1)\n",
      "Requirement already satisfied: torch in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (2.2.0+cu121)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (1.12.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (0.59.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (1.8.0)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (0.3.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from librosa) (1.0.7)\n",
      "Requirement already satisfied: filelock in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from pooch>=1.0->librosa) (4.2.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from pooch>=1.0->librosa) (23.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\manoj\\onedrive\\desktop\\deepfake-audio-detection-mfcc\\venv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa torch scikit-learn joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05fb71aa-254e-43d2-828a-765a8bc53b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import os\n",
    "import glob\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad32a92c-b154-4c2b-a3b9-65e9f686a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device to GPU if available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to extract MFCC features\n",
    "def extract_mfcc_features(audio_path, n_mfcc=25, n_fft=2048, hop_length=512):\n",
    "    try:\n",
    "        audio_data, sr = librosa.load(audio_path, sr=None)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio file {audio_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=n_mfcc, n_fft=n_fft, hop_length=hop_length)\n",
    "    delta_mfccs = librosa.feature.delta(mfccs)\n",
    "    delta_delta_mfccs = librosa.feature.delta(delta_mfccs)\n",
    "    combined_features = np.vstack([mfccs, delta_mfccs, delta_delta_mfccs])\n",
    "\n",
    "    return np.mean(combined_features.T, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60721d74-9396-4911-b06b-a22ee9aff639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create dataset\n",
    "def create_dataset(directory, label):\n",
    "    X, y = [], []\n",
    "    audio_files = glob.glob(os.path.join(directory, \"*.wav\"))\n",
    "    for audio_path in audio_files:\n",
    "        mfcc_features = extract_mfcc_features(audio_path)\n",
    "        if mfcc_features is not None:\n",
    "            X.append(mfcc_features)\n",
    "            y.append(label)\n",
    "        else:\n",
    "            print(f\"Skipping audio file {audio_path}\")\n",
    "\n",
    "    print(\"Number of samples in\", directory, \":\", len(X))\n",
    "    print(\"Filenames in\", directory, \":\", [os.path.basename(path) for path in audio_files])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6ef83be-d022-47cb-88e5-89568981525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train ensemble model\n",
    "def train_ensemble(X, y):\n",
    "    unique_classes = np.unique(y)\n",
    "    print(\"Unique classes in y_train:\", unique_classes)\n",
    "\n",
    "    if len(unique_classes) < 2:\n",
    "        raise ValueError(\"At least 2 classes are required to train\")\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Expanded Grid Search for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100, 1000],\n",
    "        'kernel': ['rbf'],\n",
    "        'gamma': ['scale', 'auto', 0.01, 0.001, 0.0001]\n",
    "    }\n",
    "    grid_search = GridSearchCV(SVC(random_state=42), param_grid, cv=5, verbose=3)\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "    # Train individual models\n",
    "    svm_classifier = SVC(**best_params, random_state=42)\n",
    "    svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "    rf_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "    lr_classifier = LogisticRegression(random_state=42)\n",
    "    lr_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "    knn_classifier = KNeighborsClassifier()\n",
    "    knn_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "    nb_classifier = GaussianNB()\n",
    "    nb_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Create an ensemble of models using soft voting\n",
    "    ensemble_classifier = VotingClassifier(estimators=[\n",
    "        ('svm', svm_classifier),\n",
    "        ('rf', rf_classifier),\n",
    "        ('lr', lr_classifier),\n",
    "        ('knn', knn_classifier),\n",
    "        ('nb', nb_classifier),\n",
    "    ], voting='soft')\n",
    "\n",
    "    ensemble_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_pred = ensemble_classifier.predict(X_test_scaled)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    confusion_mtx = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print(\"Metrics on Test Set (Ensemble):\")\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_mtx)\n",
    "\n",
    "    # Save the trained ensemble model and scaler\n",
    "    model_filename = \"ensemble_model.pkl\"\n",
    "    scaler_filename = \"scaler.pkl\"\n",
    "    joblib.dump(ensemble_classifier, model_filename)\n",
    "    joblib.dump(scaler, scaler_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1911f7b4-9b51-4809-a699-fa7fd34b80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to analyze audio\n",
    "def analyze_audio(input_audio_path):\n",
    "    model_filename = \"ensemble_model.pkl\"\n",
    "    scaler_filename = \"scaler.pkl\"\n",
    "    ensemble_classifier = joblib.load(model_filename)\n",
    "    scaler = joblib.load(scaler_filename)\n",
    "\n",
    "    if not os.path.exists(input_audio_path):\n",
    "        print(\"Error: The specified file does not exist.\")\n",
    "        return\n",
    "    elif not input_audio_path.lower().endswith(\".wav\"):\n",
    "        print(\"Error: The specified file is not a .wav file.\")\n",
    "        return\n",
    "\n",
    "    mfcc_features = extract_mfcc_features(input_audio_path)\n",
    "\n",
    "    if mfcc_features is not None:\n",
    "        mfcc_features_scaled = scaler.transform(mfcc_features.reshape(1, -1))\n",
    "        prediction = ensemble_classifier.predict(mfcc_features_scaled)\n",
    "        if prediction[0] == 0:\n",
    "            print(\"The input audio is classified as genuine.\")\n",
    "        else:\n",
    "            print(\"The input audio is classified as deepfake.\")\n",
    "    else:\n",
    "        print(\"Error: Unable to process the input audio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d5fbd0-8a18-47cc-ae1c-f8411343aa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to execute the main process\n",
    "def main():\n",
    "    genuine_dir = r\"real_audio\"\n",
    "    deepfake_dir = r\"deepfake_audio\"\n",
    "\n",
    "    X_genuine, y_genuine = create_dataset(genuine_dir, label=0)\n",
    "    X_deepfake, y_deepfake = create_dataset(deepfake_dir, label=1)\n",
    "\n",
    "    # Check if each class has at least two samples\n",
    "    if len(X_genuine) < 2 or len(X_deepfake) < 2:\n",
    "        print(\"Each class should have at least two samples for stratified splitting.\")\n",
    "        print(\"Combining both classes into one for training.\")\n",
    "        X = np.vstack((X_genuine, X_deepfake))\n",
    "        y = np.hstack((y_genuine, y_deepfake))\n",
    "    else:\n",
    "        X = np.vstack((X_genuine, X_deepfake))\n",
    "        y = np.hstack((y_genuine, y_deepfake))\n",
    "\n",
    "    train_ensemble(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e444ff4a-cf11-4692-aebe-bdc7e64fff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    user_input_file = input(\"Enter the path of the .wav file to analyze: \")\n",
    "    analyze_audio(user_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6451d81f-dab7-4e8c-b741-9c8a2524c4fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
